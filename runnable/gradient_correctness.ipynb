{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 56\n",
      "measured outcome: 1001001111 ₍₂₎\n",
      "probability of measuring the above outcome: 0.01191862824838763386814286332458357258516754616498600101682417871875535006253499\n",
      "data type used in calculations: Float64\n",
      "note: the time (μs) taken for 'iteration i' refers to the time required for the algorithm to compute p_θ(x_i|x_1,...x_{i-1}) and ∇_θ(p_θ(x_i|x_1,...x_{i-1}))\n",
      "iteration 1: 0.15009498596191406\n",
      "iteration 2: 0.025625944137573242\n",
      "iteration 3: 0.030073165893554688\n",
      "iteration 4: 0.043564796447753906\n",
      "iteration 5: 0.02081298828125\n",
      "iteration 6: 0.02620410919189453\n",
      "iteration 7: 0.08911585807800293\n",
      "iteration 8: 0.021461009979248047\n",
      "iteration 9: 0.023405075073242188\n",
      "iteration 10: 0.027287960052490234\n",
      "total time: 0.45764589309692383\n",
      "The ith entry in the following vector is p_θ(x_i|x_1,...x_{i-1})\n",
      "[0.43899921201363395, 0.7197308827565566, 0.8849153744764573, 0.9313605633823401, 0.6357015089123733, 0.4741299671024828, 0.5988245052535328, 0.5018248677965925, 0.5053245744598883, 1.0]\n",
      "the product of all entries in the above vector, should match the earlier probability computed using FLOYao.bitstring_probability: 0.011918628248387634\n",
      "The following vector is ∇_θ(log(p_θ(x))), evaluated at x = measured outcome\n",
      "[0.10092087087038887, 0.6798065966875954, -0.8790270448936839, -0.3738030143218094, -1.0064255329407354, 0.0897390759476247, 0.6428281329802226, -0.5102809159693512, 0.1666703496042421, 0.3425785939350287, 0.2998530874976961, 0.15699192715233562, -0.20129316787105253, 0.6841024613863167, 0.8715144194581189, -1.1026588984555359, -0.8140235221857535, -0.6178184346962323, -0.3650457820334253, 0.5422172300569149, -0.20094336103114047, 0.2506014418193112, 0.49392408176587993, 0.24738072943675124, 0.2076654179644484, 0.88475735802792, -0.04133039591419144, -0.46146339623851057, 0.3726015648629357, -0.3458845857460146, 0.3999608840330077, -0.15708729754609427, 1.0737047027763378, -0.3226742601067036, -0.8996184113515739, -0.2119756032504698, 0.7716418528189506, 0.5275690019640422, 0.3728833377411375, 0.2764913560869765, 0.16449803034350383, -0.882895287105675, -0.07440799568162802, -0.14367267895169733, -0.9093893800475488, 0.024588078240839417, -6.674739918528353e-18, 2.33959419824486e-17, 7.836062459860908e-18, 7.748419334825296e-18, -6.959245189227767e-17, -1.122311017586188e-16, -7.955072706288029e-17, 9.344145672980282e-17, -3.793836463263715e-17, -2.3036548444059646e-17]\n"
     ]
    }
   ],
   "source": [
    "# Constructs a quantum circuit g with parameters θ, then differentiates the recursive algorithm given in Section 5.1 of https://arxiv.org/abs/1112.2184 to obtain the gradient of p_θ(x) wrt θ, where x is a measurement of g|0>. The differentiation takes polynomial time due to memoization.\n",
    "# We then compare our results to the finite difference gradient\n",
    "\n",
    "using Yao, FLOYao\n",
    "using LinearAlgebra\n",
    "\n",
    "nq = 10 #Number of qubits\n",
    "layers = 2 #Number of brick-wall layers in the circuit\n",
    "g = chain(nq)\n",
    "for _ in 1:layers\n",
    "    for i in 1:nq-1\n",
    "        push!(g, rot(kron(nq, i => X, i+1 => X), 0.)) #Nearest-neighbor XX rotation gates\n",
    "    end\n",
    "    for i in 1:nq-1\n",
    "        push!(g, rot(kron(nq, i => X, i+1 => Y), 0.)) #Nearest-neighbor XY rotation gates\n",
    "    end\n",
    "    for i in 1:nq\n",
    "        push!(g, put(nq, i => Rz(0.))) #Single qubit Z rotation gates\n",
    "    end\n",
    "end\n",
    "\n",
    "#Set g to have random parameters\n",
    "p = rand(nparameters(g)).*2π\n",
    "dispatch!(g, p)\n",
    "nparams = nparameters(g)\n",
    "dim = 2*nq\n",
    "println(\"number of parameters: \", nparams)\n",
    "\n",
    "⊗ = kron\n",
    "\n",
    "function covariance_matrix(reg::MajoranaReg)\n",
    "    nq = nqubits(reg)\n",
    "    G = I(nq) ⊗ [0 1; -1 0]\n",
    "    return reg.state * G * reg.state'\n",
    "end\n",
    "\n",
    "function majoranaindices2kron(nq, i, j) #Returns γ_iγ_j, assuming that i≠j\n",
    "    p = []\n",
    "    c = (i % 2 == j % 2) ? 1 : -1\n",
    "    a = min(i, j)\n",
    "    b = max(i, j)\n",
    "    first = (a+1) ÷ 2 \n",
    "    last = (b+1) ÷ 2 \n",
    "    if first == last #This means i=j-1 and j is even\n",
    "        c = 1\n",
    "        push!(p, first => Z)\n",
    "    else\n",
    "        if i % 2 == 0\n",
    "            push!(p, first => X)\n",
    "            c *= 1\n",
    "        else\n",
    "            push!(p, first => Y)\n",
    "            c *= -1\n",
    "        end\n",
    "        for k in first+1:last-1\n",
    "            push!(p, k => Z)\n",
    "            c *= -1\n",
    "        end\n",
    "        if j % 2 == 0\n",
    "            push!(p, last => Y)\n",
    "        else\n",
    "            push!(p, last => X)\n",
    "        end\n",
    "    end\n",
    "    if i > j\n",
    "        c *= -1\n",
    "    end\n",
    "    return c*kron(nq, p...)\n",
    "end\n",
    "\n",
    "function majorana_commutator(nq, i, j) #Returns [γ_i,γ_j]=2γ_iγ_j, due to the anti-commutation of Majorana operators. It needs to be an 'Add' object so that the Yao.expect' function can take it in as input.\n",
    "    return Add(majoranaindices2kron(nq, i, j)) \n",
    "end\n",
    "\n",
    "function update!(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, cur_m, cur_grad_m, probabilities, grad_probabilities) #Evolves all matrices and probabilities and gradients by nq steps, in-place. This method is slow but is definitely correct. I used this to check that my more optimal function was outputting the correct thing.\n",
    "    t_tot = 0\n",
    "    for i in 1:nq\n",
    "        t = time()\n",
    "        if i > 1\n",
    "            cur_m = deepcopy(temp_m)\n",
    "            cur_grad_m = deepcopy(temp_grad_m)\n",
    "            cur_prob = deepcopy(probabilities[i-1])\n",
    "            cur_grad_prob = deepcopy(grad_probabilities[i-1, :])\n",
    "            ni = b[i-1]\n",
    "            for p in 1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_grad_m[p,q] .-= (-1)^ni * ((-cur_grad_prob * cur_m[2*(i-1)-1,p] * cur_m[2*(i-1),q]) .+ (cur_prob * (cur_grad_m[2*(i-1)-1,p]*cur_m[2*(i-1),q] .+ cur_m[2*(i-1)-1,p] * cur_grad_m[2*(i-1),q]))) / (2*cur_prob^2)\n",
    "                    temp_grad_m[p,q] .+= (-1)^ni * ((-cur_grad_prob * cur_m[2*(i-1)-1,q] * cur_m[2*(i-1),p]) .+ (cur_prob * (cur_grad_m[2*(i-1)-1,q]*cur_m[2*(i-1),p] .+ cur_m[2*(i-1)-1,q] * cur_grad_m[2*(i-1),p]))) / (2*cur_prob^2)\n",
    "                    temp_grad_m[q,p] = -temp_grad_m[p,q] \n",
    "                end\n",
    "            end\n",
    "            for p in 1:dim\n",
    "                temp_grad_m[p,p] = zeros(nparams)\n",
    "            end\n",
    "            for p in 1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_m[p,q] -= (-1)^ni * (cur_m[2*(i-1)-1,p] * cur_m[2*(i-1),q]) / (2*cur_prob)\n",
    "                    temp_m[p,q] += (-1)^ni * (cur_m[2*(i-1)-1,q] * cur_m[2*(i-1),p]) / (2*cur_prob)\n",
    "                    temp_m[q,p] = -temp_m[p,q]\n",
    "                end\n",
    "            end\n",
    "            for p in 1:dim\n",
    "                temp_m[p,p] = 0.0\n",
    "            end\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            grad_probabilities[i, :] = (-1)^ni * temp_grad_m[2*i-1, 2*i] / 2\n",
    "        else\n",
    "            dispatch!(g, theta)\n",
    "            temp_m = covariance_matrix(apply(reg, g))\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            for p in 1:dim\n",
    "                for q in p+1:dim\n",
    "                    ham = majorana_commutator(nq, p, q)\n",
    "                    temp_grad_m[p,q] = expect'(ham, reg => g)[2]\n",
    "                    temp_grad_m[q,p] = -temp_grad_m[p,q]\n",
    "                end\n",
    "            end\n",
    "            for p in 1:dim\n",
    "                temp_grad_m[p,p] = zeros(nparams)\n",
    "            end\n",
    "            grad_probabilities[i, :] = (-1)^ni * temp_grad_m[2*i-1, 2*i] / 2\n",
    "        end\n",
    "        diff = time() - t\n",
    "        t_tot += diff\n",
    "        println(\"iteration $i: $diff\")\n",
    "    end\n",
    "    println(\"total time: $t_tot\")\n",
    "end \n",
    "\n",
    "function update_opt!(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities) #Evolves all matrices and probabilities and gradients by nq steps, in-place and optimally\n",
    "    t_tot = 0\n",
    "    for i in 1:nq\n",
    "        t = time()\n",
    "        if i > 1\n",
    "            ni = b[i-1]\n",
    "            cur_prob = probabilities[i-1]\n",
    "            cur_grad_prob = grad_probabilities[:, i-1]\n",
    "            cur_prefactor = (-1)^ni / (2*cur_prob)\n",
    "            cur_grad_prefactor = (-1)^ni / (2*cur_prob^2)\n",
    "            for p in 2*(i-1)+1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_grad_m[:,p,q] .-= cur_grad_prefactor * ((-cur_grad_prob * temp_m[2*(i-1)-1,p] * temp_m[2*(i-1),q]) .+ (cur_prob * (temp_grad_m[:,2*(i-1)-1,p]*temp_m[2*(i-1),q] .+ temp_m[2*(i-1)-1,p] * temp_grad_m[:,2*(i-1),q])))\n",
    "                    temp_grad_m[:,p,q] .+= cur_grad_prefactor * ((-cur_grad_prob * temp_m[2*(i-1)-1,q] * temp_m[2*(i-1),p]) .+ (cur_prob * (temp_grad_m[:,2*(i-1)-1,q]*temp_m[2*(i-1),p] .+ temp_m[2*(i-1)-1,q] * temp_grad_m[:,2*(i-1),p])))\n",
    "                end\n",
    "            end\n",
    "            for p in 2*(i-1)+1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_m[p,q] -= cur_prefactor * (temp_m[2*(i-1)-1,p] * temp_m[2*(i-1),q])\n",
    "                    temp_m[p,q] += cur_prefactor * (temp_m[2*(i-1)-1,q] * temp_m[2*(i-1),p])\n",
    "                end\n",
    "            end\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            grad_probabilities[:, i] = (-1)^ni * temp_grad_m[:,2*i-1, 2*i] / 2\n",
    "        else\n",
    "            dispatch!(g, theta)\n",
    "            temp_m = covariance_matrix(apply(reg, g))\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            for p in 1:dim\n",
    "                for q in p+1:dim\n",
    "                    ham = majorana_commutator(nq, p, q) \n",
    "                    temp_grad_m[:,p,q] = expect'(ham, reg => g)[2]\n",
    "                end\n",
    "            end\n",
    "            grad_probabilities[:, i] = (-1)^ni * temp_grad_m[:,2*i-1, 2*i] / 2\n",
    "        end\n",
    "        diff = time() - t\n",
    "        t_tot += diff\n",
    "        println(\"iteration $i: $diff\")\n",
    "    end\n",
    "    println(\"total time: $t_tot\")\n",
    "end\n",
    "\n",
    "function log_grad(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, cur_m, cur_grad_m, probabilities, grad_probabilities) #Returns ∇_θlog(p_θ(b)), evaluated at 'theta' (parameters of circuit) and 'b' (measurement result); 'reg' is the initial register and must be of type MajoranaReg (e.g. FLOYao.zero_state(nq))\n",
    "    nq = nqubits(reg)\n",
    "    update!(reg, theta, b, temp_m, temp_grad_m, cur_m, cur_grad_m, probabilities, grad_probabilities)\n",
    "    s = zeros(length(theta))\n",
    "    for i in 1:nq\n",
    "        s += grad_probabilities[i, :] / probabilities[i]\n",
    "    end\n",
    "    basic_prob = probabilities\n",
    "    return basic_prob, s\n",
    "end\n",
    "\n",
    "function log_grad_opt(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities) #Returns ∇_θlog(p_θ(b)), evaluated at 'theta' (parameters of circuit) and 'b' (measurement result); 'reg' is the initial register and must be of type MajoranaReg (e.g. FLOYao.zero_state(nq)). This uses the optimal updating function which is more efficient but still outputs the same thing as the original update! function.\n",
    "    update_opt!(reg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "    s = zeros(length(theta))\n",
    "    for i in 1:nq\n",
    "        s += grad_probabilities[:, i] / probabilities[i]\n",
    "    end\n",
    "    optimized_prob = probabilities\n",
    "    return optimized_prob, s\n",
    "end\n",
    "\n",
    "reg = apply(FLOYao.zero_state(nq), g)\n",
    "bitstr = measure(reg, nshots = 1)[1] #Random measurement of g|0>\n",
    "println(\"measured outcome: $bitstr\")\n",
    "println(\"probability of measuring the above outcome: \", FLOYao.bitstring_probability(reg, bitstr)) #Uses FLOYao.bitstring_probability(reg, bitstr) which is known to be correct. We check this number against our algorithm output, to verify correctness.\n",
    "\n",
    "T = Float64 #Can also be BigFloat, may experiment with other data types later\n",
    "println(\"data type used in calculations: $T\") \n",
    "println(\"note: the time (μs) taken for 'iteration i' refers to the time required for the algorithm to compute p_θ(x_i|x_1,...x_{i-1}) and ∇_θ(p_θ(x_i|x_1,...x_{i-1}))\")\n",
    "\n",
    "#Initializing temporary matrices and vectors used in the unoptimal version of the algorithm.\n",
    "temp_m = Matrix{T}(undef, dim, dim)\n",
    "temp_grad_m = Matrix{Vector{T}}(undef, dim, dim)\n",
    "cur_m = Matrix{T}(undef, dim, dim)\n",
    "cur_grad_m = Matrix{Vector{T}}(undef, dim, dim)\n",
    "probabilities = Vector{T}(undef, nq)\n",
    "grad_probabilities = Matrix{T}(undef, nq, nparams)\n",
    "\n",
    "basic_prob, basic = log_grad(FLOYao.zero_state(nq), p, bitstr, temp_m, temp_grad_m, cur_m, cur_grad_m, probabilities, grad_probabilities)\n",
    "println(\"The ith entry in the following vector is p_θ(x_i|x_1,...x_{i-1})\")\n",
    "println(basic_prob)\n",
    "println(\"the product of all entries in the above vector, should match the earlier probability computed using FLOYao.bitstring_probability: \", prod(basic_prob))\n",
    "println(\"The following vector is ∇_θ(log(p_θ(x))), evaluated at x = measured outcome\")\n",
    "println(basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1: 0.02676987648010254\n",
      "iteration 2: 0.011457204818725586\n",
      "iteration 3: 0.013464927673339844\n",
      "iteration 4: 0.00638890266418457\n",
      "iteration 5: 0.004697084426879883\n",
      "iteration 6: 0.003204822540283203\n",
      "iteration 7: 0.0020678043365478516\n",
      "iteration 8: 0.0011260509490966797\n",
      "iteration 9: 0.0004799365997314453\n",
      "iteration 10: 0.00011515617370605469\n",
      "total time: 0.06977176666259766\n",
      "probabilities equal? true\n",
      "grad(log p) equal? true\n",
      "The following vector is ∇_θ(log(p_θ(x))), evaluated at x = measured outcome, as outputted by the optimized version of the algorithm.\n",
      "[0.10092087087038887, 0.6798065966875954, -0.8790270448936839, -0.3738030143218094, -1.0064255329407354, 0.0897390759476247, 0.6428281329802226, -0.5102809159693512, 0.1666703496042421, 0.3425785939350287, 0.2998530874976961, 0.15699192715233562, -0.20129316787105253, 0.6841024613863167, 0.8715144194581189, -1.1026588984555359, -0.8140235221857535, -0.6178184346962323, -0.3650457820334253, 0.5422172300569149, -0.20094336103114047, 0.2506014418193112, 0.49392408176587993, 0.24738072943675124, 0.2076654179644484, 0.88475735802792, -0.04133039591419144, -0.46146339623851057, 0.3726015648629357, -0.3458845857460146, 0.3999608840330077, -0.15708729754609427, 1.0737047027763378, -0.3226742601067036, -0.8996184113515739, -0.2119756032504698, 0.7716418528189506, 0.5275690019640422, 0.3728833377411375, 0.2764913560869765, 0.16449803034350383, -0.882895287105675, -0.07440799568162802, -0.14367267895169733, -0.9093893800475488, 0.024588078240839417, -6.674739918528353e-18, 2.33959419824486e-17, 7.836062459860908e-18, 7.748419334825296e-18, -6.959245189227767e-17, -1.122311017586188e-16, -7.955072706288029e-17, 9.344145672980282e-17, -3.793836463263715e-17, -2.3036548444059646e-17]\n"
     ]
    }
   ],
   "source": [
    "#Initializing temporary matrices and vectors for the optimized version of the algorithm.\n",
    "temp_m = Matrix{T}(undef, dim, dim)\n",
    "temp_grad_m = Array{T}(undef, nparams, dim, dim)\n",
    "probabilities = Vector{T}(undef, nq)\n",
    "grad_probabilities = Matrix{T}(undef, nparams, nq)\n",
    "\n",
    "#Calling the optimized version of the algorithm. 'optimized_prob' represents the vector with ith entry p_θ(x_i|x_1,...x_{i-1}) and 'optimized' is ∇_θ(log(p_θ(x))).\n",
    "optimized_prob, optimized = log_grad_opt(FLOYao.zero_state(nq), p, bitstr, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "\n",
    "#Checking that the output of the optimized algorithm still matches the output of the unoptimized algorithm.\n",
    "println(\"probabilities equal? \", basic_prob == optimized_prob)\n",
    "println(\"grad(log p) equal? \", basic == optimized)\n",
    "println(\"The following vector is ∇_θ(log(p_θ(x))), evaluated at x = measured outcome, as outputted by the optimized version of the algorithm.\")\n",
    "println(optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43899921201363395, 0.7197308827565566, 0.8849153744764573, 0.9313605633823401, 0.6357015089123733, 0.4741299671024828, 0.5988245052535328, 0.5018248677965925, 0.5053245744598883, 1.0]\n",
      "iteration 1: 0.1521899700164795\n",
      "iteration 2: 0.01124882698059082\n",
      "iteration 3: 0.010317087173461914\n",
      "iteration 4: 0.009216070175170898\n",
      "iteration 5: 0.00569605827331543\n",
      "iteration 6: 0.012009143829345703\n",
      "iteration 7: 0.0022809505462646484\n",
      "iteration 8: 0.0022170543670654297\n",
      "iteration 9: 0.0005359649658203125\n",
      "iteration 10: 0.00016117095947265625\n",
      "total time: 0.2058722972869873\n",
      "true\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56-element Vector{Float64}:\n",
       "  0.10092087087038887\n",
       "  0.6798065966875954\n",
       " -0.8790270448936839\n",
       " -0.3738030143218094\n",
       " -1.0064255329407354\n",
       "  0.0897390759476247\n",
       "  0.6428281329802226\n",
       " -0.5102809159693512\n",
       "  0.1666703496042421\n",
       "  0.3425785939350287\n",
       "  ⋮\n",
       "  2.33959419824486e-17\n",
       "  7.836062459860908e-18\n",
       "  7.748419334825296e-18\n",
       " -6.959245189227767e-17\n",
       " -1.122311017586188e-16\n",
       " -7.955072706288029e-17\n",
       "  9.344145672980282e-17\n",
       " -3.793836463263715e-17\n",
       " -2.3036548444059646e-17"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "println(probabilities)\n",
    "a, b = log_grad_opt(FLOYao.zero_state(nq), p, bitstr, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "println(b == optimized)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "algorithm output for ∇_θ(log(p_θ(x))), should be exact\n",
      "[0.10092087087038887, 0.6798065966875954, -0.8790270448936839, -0.3738030143218094, -1.0064255329407354, 0.0897390759476247, 0.6428281329802226, -0.5102809159693512, 0.1666703496042421, 0.3425785939350287, 0.2998530874976961, 0.15699192715233562, -0.20129316787105253, 0.6841024613863167, 0.8715144194581189, -1.1026588984555359, -0.8140235221857535, -0.6178184346962323, -0.3650457820334253, 0.5422172300569149, -0.20094336103114047, 0.2506014418193112, 0.49392408176587993, 0.24738072943675124, 0.2076654179644484, 0.88475735802792, -0.04133039591419144, -0.46146339623851057, 0.3726015648629357, -0.3458845857460146, 0.3999608840330077, -0.15708729754609427, 1.0737047027763378, -0.3226742601067036, -0.8996184113515739, -0.2119756032504698, 0.7716418528189506, 0.5275690019640422, 0.3728833377411375, 0.2764913560869765, 0.16449803034350383, -0.882895287105675, -0.07440799568162802, -0.14367267895169733, -0.9093893800475488, 0.024588078240839417, -6.674739918528353e-18, 2.33959419824486e-17, 7.836062459860908e-18, 7.748419334825296e-18, -6.959245189227767e-17, -1.122311017586188e-16, -7.955072706288029e-17, 9.344145672980282e-17, -3.793836463263715e-17, -2.3036548444059646e-17]\n",
      "finite difference approximation to ∇_θ(log(p_θ(x)))\n",
      "[0.10092086515807965, 0.6798065961129418, -0.879027023093521, -0.37380300029281277, -1.0064255044703325, 0.08973909039924403, 0.642828126275324, -0.510280905129837, 0.16667034982514242, 0.34257856340136167, 0.29985308835320335, 0.15699191656466432, -0.201293167693948, 0.6841024694723213, 0.8715144072798164, -1.1026589099474775, -0.8140235232440756, -0.6178184274060592, -0.36504577578112163, 0.542217247377644, -0.20094338320115, 0.25060142982677774, 0.4939240773561083, 0.2473806961350298, 0.20766541161045324, 0.8847573672097762, -0.04133040450512287, -0.4614633839050887, 0.3726015840078495, -0.34588459659396714, 0.3999608908705271, -0.1570872828424354, 1.073704714457225, -0.32267424079928847, -0.8996184111447413, -0.21197561968871464, 0.7716418647618227, 0.5275690103000208, 0.3728833146645621, 0.27649137746978125, 0.1644980178238195, -0.8828952845188598, -0.07440800265445421, -0.1436726923791102, -0.9093893899401341, 0.024588078395066063, 8.436445445127257e-10, 1.4541121883253656e-8, 9.796497134419113e-9, -9.658229798910143e-9, 3.3416850684234116e-9, -3.3106201454924206e-9, 2.251492380447321e-9, -1.0663718026959606e-8, -1.2288418356588312e-9, -5.551115123125783e-9]\n",
      "l2 distance between algorithm output and finite difference approximation: \n",
      "9.787232104820016e-8\n"
     ]
    }
   ],
   "source": [
    "#Comparison with finite difference method\n",
    "using LinearAlgebra\n",
    "\n",
    "function prob(theta, x) #Outputs p_θ(x), the probability of measuring an outcome of 'x' for the state g|0> where the parameters of g are set to 'theta'\n",
    "    circuit = dispatch(g, theta)\n",
    "    r = apply(FLOYao.zero_state(nq), circuit)\n",
    "    return FLOYao.bitstring_probability(r, x)\n",
    "end\n",
    "\n",
    "eps_default = 1e-8\n",
    "function fe_grad_prob(theta, x, eps = eps_default) #Computes the finite-difference approximation for ∇_θ(log(p_θ(x))), evaluated at 'x'. I mainly used this to verify the correctness of the algorithm\n",
    "    temp_params = copy(theta)\n",
    "    fe_grad = Vector{Float64}(undef, length(theta))\n",
    "    for i in 1:nparameters(g)\n",
    "        temp_params[i] += eps\n",
    "        plus = log(prob(temp_params, x))\n",
    "        temp_params[i] -= 2*eps\n",
    "        minus = log(prob(temp_params, x))\n",
    "        fe_grad[i] = (plus - minus) / (2*eps)\n",
    "        temp_params[i] += eps #Resetting temp_params[i] back to original value\n",
    "    end\n",
    "    fe_grad\n",
    "end\n",
    "\n",
    "println(\"algorithm output for ∇_θ(log(p_θ(x))), should be exact\")\n",
    "println(basic)\n",
    "fe = fe_grad_prob(p, bitstr)\n",
    "println(\"finite difference approximation to ∇_θ(log(p_θ(x)))\")\n",
    "println(fe)\n",
    "println(\"l2 distance between algorithm output and finite difference approximation: \")\n",
    "println(norm(basic - fe))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
