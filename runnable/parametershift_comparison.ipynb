{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters in critic: 121\n",
      "Number of parameters in generator: 56\n",
      "measured outcome: 1000111011 ₍₂₎\n",
      "probability of measuring the above outcome: 0.0009722767794702614632794779696618557050922010789304731045384289689020436863483963\n",
      "data type used in calculations: Float64\n",
      "note: the time (μs) taken for 'iteration i' refers to the time required for the algorithm to compute p_θ(x_i|x_1,...x_{i-1}) and ∇_θ(p_θ(x_i|x_1,...x_{i-1}))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Vector{Float64}\u001b[90m (alias for \u001b[39m\u001b[90mArray{Float64, 1}\u001b[39m\u001b[90m)\u001b[39m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Constructs a quantum circuit g with parameters θ, then differentiates the recursive algorithm given in Section 5.1 of https://arxiv.org/abs/1112.2184 to obtain the gradient of p_θ(x) wrt θ, where x is a measurement of g|0>. The differentiation takes polynomial time due to memoization.\n",
    "# We then compare our results to the finite difference gradient\n",
    "\n",
    "using Yao, FLOYao\n",
    "using LinearAlgebra, Statistics\n",
    "using Flux\n",
    "\n",
    "nq = 10\n",
    "\n",
    "d = Chain(Dense(nq, 10, relu), Dense(10, 1, sigmoid))\n",
    "nparams = sum(length, Flux.params(d))\n",
    "println(\"Number of parameters in critic: $nparams\")\n",
    "\n",
    "layers = 2 #Number of brick-wall layers in the circuit\n",
    "g = chain(nq)\n",
    "for _ in 1:layers\n",
    "    for i in 1:nq-1\n",
    "        push!(g, rot(kron(nq, i => X, i+1 => X), 0.)) #Nearest-neighbor XX rotation gates\n",
    "    end\n",
    "    for i in 1:nq-1\n",
    "        push!(g, rot(kron(nq, i => X, i+1 => Y), 0.)) #Nearest-neighbor XY rotation gates\n",
    "    end\n",
    "    for i in 1:nq\n",
    "        push!(g, put(nq, i => Rz(0.))) #Single qubit Z rotation gates\n",
    "    end\n",
    "end\n",
    "\n",
    "#Set g to have random parameters\n",
    "p = rand(nparameters(g)).*2π\n",
    "dispatch!(g, p)\n",
    "nparams = nparameters(g)\n",
    "dim = 2*nq\n",
    "println(\"Number of parameters in generator: $nparams\")\n",
    "\n",
    "⊗ = kron\n",
    "\n",
    "function covariance_matrix(reg::MajoranaReg)\n",
    "    nq = nqubits(reg)\n",
    "    G = I(nq) ⊗ [0 1; -1 0]\n",
    "    return reg.state * G * reg.state'\n",
    "end\n",
    "\n",
    "function majoranaindices2kron(nq, i, j) #Returns γ_iγ_j, assuming that i≠j\n",
    "    p = []\n",
    "    c = (i % 2 == j % 2) ? 1 : -1\n",
    "    a = min(i, j)\n",
    "    b = max(i, j)\n",
    "    first = (a+1) ÷ 2 \n",
    "    last = (b+1) ÷ 2 \n",
    "    if first == last #This means i=j-1 and j is even\n",
    "        c = 1\n",
    "        push!(p, first => Z)\n",
    "    else\n",
    "        if i % 2 == 0\n",
    "            push!(p, first => X)\n",
    "            c *= 1\n",
    "        else\n",
    "            push!(p, first => Y)\n",
    "            c *= -1\n",
    "        end\n",
    "        for k in first+1:last-1\n",
    "            push!(p, k => Z)\n",
    "            c *= -1\n",
    "        end\n",
    "        if j % 2 == 0\n",
    "            push!(p, last => Y)\n",
    "        else\n",
    "            push!(p, last => X)\n",
    "        end\n",
    "    end\n",
    "    if i > j\n",
    "        c *= -1\n",
    "    end\n",
    "    return c*kron(nq, p...)\n",
    "end\n",
    "\n",
    "function majorana_commutator(nq, i, j) #Returns [γ_i,γ_j]=2γ_iγ_j, due to the anti-commutation of Majorana operators. It needs to be an 'Add' object so that the Yao.expect' function can take it in as input.\n",
    "    return Add(majoranaindices2kron(nq, i, j)) \n",
    "end\n",
    "\n",
    "function update_opt!(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities) #Evolves all matrices and probabilities and gradients by nq steps, in-place and optimally\n",
    "    t_tot = 0\n",
    "    for i in 1:nq\n",
    "        t = time()\n",
    "        if i > 1\n",
    "            ni = b[i-1]\n",
    "            cur_prob = probabilities[i-1]\n",
    "            cur_grad_prob = grad_probabilities[:, i-1]\n",
    "            cur_prefactor = (-1)^ni / (2*cur_prob)\n",
    "            cur_grad_prefactor = (-1)^ni / (2*cur_prob^2)\n",
    "            for p in 2*(i-1)+1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_grad_m[:,p,q] .-= cur_grad_prefactor * ((-cur_grad_prob * temp_m[2*(i-1)-1,p] * temp_m[2*(i-1),q]) .+ (cur_prob * (temp_grad_m[:,2*(i-1)-1,p]*temp_m[2*(i-1),q] .+ temp_m[2*(i-1)-1,p] * temp_grad_m[:,2*(i-1),q])))\n",
    "                    temp_grad_m[:,p,q] .+= cur_grad_prefactor * ((-cur_grad_prob * temp_m[2*(i-1)-1,q] * temp_m[2*(i-1),p]) .+ (cur_prob * (temp_grad_m[:,2*(i-1)-1,q]*temp_m[2*(i-1),p] .+ temp_m[2*(i-1)-1,q] * temp_grad_m[:,2*(i-1),p])))\n",
    "                end\n",
    "            end\n",
    "            for p in 2*(i-1)+1:dim\n",
    "                for q in p+1:dim\n",
    "                    temp_m[p,q] -= cur_prefactor * (temp_m[2*(i-1)-1,p] * temp_m[2*(i-1),q])\n",
    "                    temp_m[p,q] += cur_prefactor * (temp_m[2*(i-1)-1,q] * temp_m[2*(i-1),p])\n",
    "                end\n",
    "            end\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            grad_probabilities[:, i] = (-1)^ni * temp_grad_m[:,2*i-1, 2*i] / 2\n",
    "        else\n",
    "            dispatch!(g, theta)\n",
    "            temp_m = covariance_matrix(apply(reg, g))\n",
    "            ni = b[i]\n",
    "            probabilities[i] = (1+(-1)^ni * temp_m[2*i-1, 2*i]) / 2\n",
    "            for p in 1:dim\n",
    "                for q in p+1:dim\n",
    "                    ham = majorana_commutator(nq, p, q)\n",
    "                    temp_grad_m[:,p,q] = expect'(ham, reg => g)[2]\n",
    "                end\n",
    "            end\n",
    "            grad_probabilities[:, i] = (-1)^ni * temp_grad_m[:,2*i-1, 2*i] / 2\n",
    "        end\n",
    "        # diff = time() - t\n",
    "        # t_tot += diff\n",
    "        # println(\"iteration $i: $diff\")\n",
    "    end\n",
    "    # println(\"total time: $t_tot\")\n",
    "end\n",
    "\n",
    "function log_grad_opt(reg::MajoranaReg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities) #Returns ∇_θlog(p_θ(b)), evaluated at 'theta' (parameters of circuit) and 'b' (measurement result); 'reg' is the initial register and must be of type MajoranaReg (e.g. FLOYao.zero_state(nq)). This uses the optimal updating function which is more efficient but still outputs the same thing as the original update! function.\n",
    "    update_opt!(reg, theta, b, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "    s = zeros(length(theta))\n",
    "    for i in 1:nq\n",
    "        s += grad_probabilities[:, i] / probabilities[i]\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "reg = apply(FLOYao.zero_state(nq), g)\n",
    "bitstr = measure(reg, nshots = 1)[1] #Random measurement of g|0>\n",
    "println(\"measured outcome: $bitstr\")\n",
    "println(\"probability of measuring the above outcome: \", FLOYao.bitstring_probability(reg, bitstr)) #Uses FLOYao.bitstring_probability(reg, bitstr) which is known to be correct. We check this number against our algorithm output, to verify correctness.\n",
    "\n",
    "T = Float64 #Can also be BigFloat, may experiment with other data types later\n",
    "println(\"data type used in calculations: $T\") \n",
    "println(\"note: the time (μs) taken for 'iteration i' refers to the time required for the algorithm to compute p_θ(x_i|x_1,...x_{i-1}) and ∇_θ(p_θ(x_i|x_1,...x_{i-1}))\")\n",
    "\n",
    "#Initializing temporary matrices and vectors for the optimized version of the algorithm.\n",
    "temp_m = Matrix{T}(undef, dim, dim)\n",
    "temp_grad_m = Array{T}(undef, nparams, dim, dim)\n",
    "probabilities = Vector{T}(undef, nq)\n",
    "grad_probabilities = Matrix{T}(undef, nparams, nq)\n",
    "\n",
    "#Calling the optimized version of the algorithm. 'optimized_prob' represents the vector with ith entry p_θ(x_i|x_1,...x_{i-1}) and 'optimized' is ∇_θ(log(p_θ(x))).\n",
    "optimized = log_grad_opt(FLOYao.zero_state(nq), p, bitstr, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "typeof(optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1×5 Matrix{Float32}:\n",
       " 0.63874  0.786409  0.647613  0.728816  0.54574"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "using BitBasis\n",
    "\n",
    "batchsize = 5\n",
    "\n",
    "function postprocess(g_output::Vector) #turns output of measure  into an Int vector\n",
    "    result = []\n",
    "    for i in 1:N \n",
    "        push!(result, g_output[1][end - i + 1])\n",
    "    end\n",
    "    Int.(result)\n",
    "end\n",
    "\n",
    "function d_postprocess(measurement::Vector, nbatch = batchsize)\n",
    "    aa = breflect.(measurement)\n",
    "    ret = Matrix(undef, nq, nbatch)\n",
    "    for i in 1:nbatch\n",
    "        ret[:,i] = [aa[i]...]\n",
    "    end\n",
    "    return ret\n",
    "end\n",
    "\n",
    "result = measure(reg, nshots = batchsize)\n",
    "d(d_postprocess(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}Vector{Float64}"
     ]
    },
    {
     "data": {
      "text/plain": [
       "56-element Vector{Float64}:\n",
       "  0.07646864568400227\n",
       " -0.045437800373105006\n",
       " -0.0471501014902149\n",
       " -0.040258450629033174\n",
       "  0.010351659557891472\n",
       "  0.1595348953432531\n",
       "  0.037587866215170636\n",
       " -0.010715848590731106\n",
       " -0.018285958541070523\n",
       "  0.1076571555755077\n",
       "  ⋮\n",
       " -3.4625162655894115e-18\n",
       " -8.813611269947807e-19\n",
       " -2.2922056251062456e-18\n",
       " -5.633473541879167e-19\n",
       "  3.879159003495091e-18\n",
       "  5.735920267958036e-18\n",
       " -7.672212537684256e-18\n",
       "  2.2829769287990792e-18\n",
       "  1.1318048220693131e-18"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function reinforce_grad_loss(theta, nbatch)\n",
    "    dispatch!(g, theta)\n",
    "    T = Float64\n",
    "    sampled = Dict{BitStr{nq, BigInt}, Vector{T}}()\n",
    "    measurements = measure(reg, nshots = nbatch)\n",
    "    discriminator_output = log.(d(d_postprocess(measurements, nbatch)))\n",
    "    print(length(discriminator_output))\n",
    "    #Initializing temporary matrices and vectors for the optimized version of the algorithm. Note: Do NOT need to reset these temporary matrices at the end of each iteration of the for loop.\n",
    "    temp_m = Matrix{T}(undef, dim, dim)\n",
    "    temp_grad_m = Array{T}(undef, nparams, dim, dim)\n",
    "    probabilities = Vector{T}(undef, nq)\n",
    "    grad_probabilities = Matrix{T}(undef, nparams, nq)\n",
    "    grad_p = Matrix{T}(undef, nparams, nbatch)\n",
    "    for i in 1:nbatch\n",
    "        cur_bitstr = measurements[i]\n",
    "        if haskey(sampled, cur_bitstr)\n",
    "            # println(\"SAMPLED AGAIN\")\n",
    "            grad_p[:,i] = sampled[cur_bitstr]\n",
    "        else\n",
    "            log_grad = log_grad_opt(FLOYao.zero_state(nq), theta, cur_bitstr, temp_m, temp_grad_m, probabilities, grad_probabilities)\n",
    "            print(typeof(log_grad))\n",
    "            grad_p[:,i] = log_grad\n",
    "            sampled[cur_bitstr] = log_grad\n",
    "        end\n",
    "    end\n",
    "    return vec(mean(discriminator_output.*grad_p, dims = 2))\n",
    "end\n",
    "\n",
    "reinforce_grad_loss(p, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using LinearAlgebra\n",
    "\n",
    "# asdf = reinforce_grad_loss(p, 100)\n",
    "# ddasf = reinforce_grad_loss(p, 1000)\n",
    "# println(asdf)\n",
    "# println(ddasf)\n",
    "# println(norm(asdf, ddasf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Parameter shift gradient\n",
    "# function gLoss(nbatch = 100)\n",
    "#     measurements = measure(reg, nshots = nbatch)\n",
    "#     return mean(-log(measurements))\n",
    "# end\n",
    "# mean(f(run_g(theta = theta, nbatch = nbatch)))\n",
    "\n",
    "# function parametershift_grad(theta = parameters(g)) #Shift parameters in-place\n",
    "#     temp_params = theta\n",
    "#     temp_params[1] += π/2\n",
    "#     grad = Vector{Float64}(undef, nparams)\n",
    "#     for i in 1:l\n",
    "#         plus = 0\n",
    "#         minus = 0\n",
    "#         dispatch!(g, temp_params)\n",
    "#         plus = gLoss()\n",
    "#         temp_params[i] -= π\n",
    "#         dispatch!(g, temp_params)\n",
    "#         minus = gLoss()\n",
    "#         grad[i] = (plus - minus) / 2\n",
    "#         if i < l\n",
    "#             temp_params[i] += π/2\n",
    "#             temp_params[i+1] += π/2\n",
    "#         end\n",
    "#     end\n",
    "#     temp_params[l] += π/2\n",
    "#     dispatch!(g, temp_params)\n",
    "#     return grad\n",
    "# end\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
