{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using Pkg\n",
    "# Pkg.activate(\"\")\n",
    "\n",
    "# # Packages that you need\n",
    "# packages_to_install = [\"MLDatasets\", \"Plots\", \"Statistics\", \"Distributions\", \"Random\", \"Flux\", \"Yao\", \"FLOYao\", \"PythonCall\", \"BenchmarkTools\", \"Dates\", \"FiniteDifferences\"]\n",
    "\n",
    "# for pkg in packages_to_install\n",
    "#     Pkg.add(pkg)\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLDatasets\n",
    "using Plots\n",
    "using Statistics\n",
    "using Distributions\n",
    "using Random\n",
    "\n",
    "train = 4 # Size of training dataset\n",
    "\n",
    "digit = 4 # Digit to be learned\n",
    "image_size = 28 # Size of image\n",
    "compress_size = 4 # Size of compressed image\n",
    "\n",
    "function compress(img, original, compress, max = true) #If max is false, then it does mean-convolution instead\n",
    "    ratio = original ÷ compress\n",
    "    compressed = zeros(compress, compress)\n",
    "    a = []\n",
    "    for i in 0:compress-1\n",
    "        for j in 0:compress-1\n",
    "            empty!(a)\n",
    "            for p in 1:ratio\n",
    "                for q in 1:ratio\n",
    "                    push!(a, img[(ratio*i) + p, (ratio*j) + q])\n",
    "                end\n",
    "            end\n",
    "            compressed[i+1, j+1] = max ? maximum(a) : mean(a)\n",
    "        end\n",
    "    end\n",
    "    return compressed\n",
    "end\n",
    "\n",
    "train_set = MNIST(:train)\n",
    "train_labels = MNIST(split=:train).targets\n",
    "\n",
    "data_train = []\n",
    "c = 1\n",
    "trainstats = []\n",
    "for _ in 1:train\n",
    "    while train_labels[c] != digit\n",
    "        c += 1\n",
    "    end\n",
    "    push!(data_train, compress(rotl90(train_set[c][1]), image_size, compress_size) .> 0.5)\n",
    "    push!(trainstats, sum(compress(train_set[c][1], image_size, compress_size) .> 0.5))\n",
    "    c += 1\n",
    "end\n",
    "\n",
    "plot([heatmap(x, color=:grays, colorbar = false, ticks = false) for x in data_train]...) #Is there any way to make this look nicer? oh well it's not a big deal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Yao\n",
    "using FLOYao\n",
    "using Flux\n",
    "\n",
    "N = compress_size^2 #Number of qubits\n",
    "\n",
    "function NeuralNetwork(num_dim = N, random = true)\n",
    "    net = Chain(Dense(num_dim, 25, relu), Dense(25, 1, identity))\n",
    "    if !random\n",
    "        for p in Flux.params(net)\n",
    "            p .= (ones(size(p)) .* 0.08)\n",
    "        end\n",
    "    end\n",
    "    return net\n",
    "end\n",
    "\n",
    "#d = NeuralNetwork() #Parameters are random (typically is N(0, sqrt(width)), not really sure how it's implemented)\n",
    "d = Chain(Dense(N, 25, relu), Dense(25, 1, sigmoid)) #I have no clue what this looks like, and why does it have more parameters than expected (bias?)\n",
    "\n",
    "nparams = sum(length, Flux.params(d))\n",
    "println(\"Number of parameters in discriminator: $nparams\")\n",
    "\n",
    "layers = 5\n",
    "g = chain(N) #Making the generator, which is a matchgate ansatz (IS IT???) yes because it's made up of nn XX gates and single-qubit Z gates\n",
    "for _ in 1:layers\n",
    "    for i in 1:N-1\n",
    "        push!(g, rot(kron(N, i => X, i+1 => X), 0.))\n",
    "    end\n",
    "    for i in 1:N\n",
    "        push!(g, put(N, i => Rz(0.)))\n",
    "    end\n",
    "end\n",
    "\n",
    "# function build_circuit(n::Int = N, nlayers::Int = layers)\n",
    "#     circuit = chain(n) #Making the generator, which is a matchgate ansatz; this just uses Yao though? or does saying \"using FLOYao\" somehow change the implementation... Not really sure\n",
    "#     for _ in 1:nlayers\n",
    "#         for i in 1:N-1\n",
    "#             push!(circuit, rot(kron(N, i => X, i+1 => X), 0.))\n",
    "#         end\n",
    "#         for i in 1:N\n",
    "#             push!(circuit, put(N, i => Rz(0.)))\n",
    "#         end\n",
    "#     end\n",
    "# end\n",
    "\n",
    "#g = build_circuit()\n",
    "#g = build_circuit() |> autodiff(:QC)\n",
    "\n",
    "nparams = nparameters(g)\n",
    "println(\"Number of parameters in generator: $nparams\")  \n",
    "#dispatch!(g, ones(nparams) ./ 100); \n",
    "dispatch!(g, :random);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using FLOYao\n",
    "using Random\n",
    "using Distributions\n",
    "\n",
    "#Not sure if there is a better pprior that we can choose, sometimes u get a lot of white or black bc variance is high (resolution: change var from sigma to sigma/10)\n",
    "mu = mean(trainstats)\n",
    "sigma = std(trainstats)\n",
    "println(\"μ: $mu\")\n",
    "println(\"σ: $sigma\")\n",
    "#dist = Normal(mu, sigma/20)\n",
    "dist = Normal(mu, 1)\n",
    "\n",
    "ratio = rand(dist) / N\n",
    "function pprior() #For each square, probability ratio that it's white, where ratio ~ N(mean(data), std(data)) /  N; maybe we can vary pprior later?\n",
    "    a = rand(N) .< ratio\n",
    "    return FLOYao.product_state(Int.(a)) #Returns MajoranaReg; Prepares it to be sent through the generator \n",
    "end\n",
    "\n",
    "pprior_example = rand(N) .< ratio\n",
    "heatmap(rotl90(reshape(pprior_example, compress_size, compress_size)) .> 0.5, color=:grays, colorbar = false, ticks = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux: train!\n",
    "\n",
    "batch_size = train #Are you supposed to randomly pick batch_size data points at random each time if train is big? idk\n",
    "\n",
    "#g takes as input x = Int.(pprior()); FLOYao.product_state(x)\n",
    "#d takes as input measure(x |> g)\n",
    "\n",
    "function convertgd(g_output::Vector) #turns the output of reg |> g |> measure into an Int vector\n",
    "    result = []\n",
    "    for i in 1:N \n",
    "        push!(result, g_output[1][end - i + 1])\n",
    "    end\n",
    "    Int.(result)\n",
    "end\n",
    "\n",
    "function run_circuit(z::MajoranaReg) #z is a MajoranaReg drawn from pprior (to guarantee matchgate speedup); this function applies D(G(z))\n",
    "    g_output = z |> g |> measure\n",
    "    d_input = convertgd(g_output)\n",
    "    d_output = d_input |> d\n",
    "    d_output |> first\n",
    "end\n",
    "\n",
    "t = time()\n",
    "measurement = run_circuit(pprior())\n",
    "println(\"runcircuit: $measurement\")\n",
    "println(time() - t)\n",
    "\n",
    "function run_d(x::BitMatrix) #this function applies D(x), where x is an element of data_train \n",
    "    d_output = x |> vec |> d\n",
    "    d_output |> first\n",
    "end\n",
    "\n",
    "function gLoss(z::MajoranaReg) #z is sampled from pprior()\n",
    "    -log(run_circuit(z))\n",
    "end\n",
    "\n",
    "function dLoss(m, real, fake) #m is the discriminator, real is the actual data, fake is G(z) where z is sampled from pprior()\n",
    "    -(log.(m(real)) + log.(1 .- m(fake))) |> first\n",
    "end\n",
    "\n",
    "# function dLoss(m, fake)\n",
    "#     -(log.(m(real)) + log.(1 .- m(fake))) |> first\n",
    "# end\n",
    "\n",
    "# Worry about this another day\n",
    "# function gradient(op, reg::MajoranaReg) #does expect' (from FLOYao? or does it come from Yao?) use AD? Also idk what op should be\n",
    "#     _, params_grad = expect'(op, reg => g)\n",
    "# end\n",
    "ε = 1e-6 #FIGURE OUT HOW TO USE FINITE DIFFERENECES PACKAGE\n",
    "function finitediff_grad(z::MajoranaReg, eps = ε) #Returns the approximate gradient of G, using forward finite differences\n",
    "    x = gLoss(z)\n",
    "    temp_params = parameters(g)\n",
    "    grad = zeros(nparameters(g))\n",
    "    temp = 0\n",
    "    for i in 1:nparameters(g)\n",
    "        temp = parameters(g)[i]\n",
    "        temp_params[i] = temp + eps\n",
    "        dispatch!(g, temp_params)\n",
    "        plus = gLoss(z)\n",
    "        grad[i] = (plus - x) / eps\n",
    "        temp_params[i] = temp\n",
    "        dispatch!(g, temp_params)\n",
    "    end\n",
    "    grad\n",
    "end\n",
    "#dispatch! takes a long time, and when you're doing it 2*nparameters(g) times...\n",
    "\n",
    "# function finitediff_grad(z::MajoranaReg)\n",
    "# end\n",
    "\n",
    "# function finitediff_grad(z::MajoranaReg, eps = ε) #Returns the approximate gradient of G, using central finite differences\n",
    "#     temp_params = parameters(g)\n",
    "#     grad = zeros(nparameters(g))\n",
    "#     temp = plus = minus = 0\n",
    "#     for i in 1:nparameters(g)\n",
    "#         temp = parameters(g)[i]\n",
    "#         temp_params[i] = temp + eps\n",
    "#         dispatch!(g, temp_params)\n",
    "#         plus = gLoss(z)\n",
    "#         temp_params[i] = temp - eps\n",
    "#         dispatch!(g, temp_params)\n",
    "#         minus = gLoss(z)\n",
    "#         grad[i] = (plus - minus) / (2*eps)\n",
    "#         temp_params[i] = temp\n",
    "#         dispatch!(g, temp_params)\n",
    "#     end\n",
    "#     grad\n",
    "# end\n",
    "\n",
    "z = pprior()\n",
    "@time finitediff_grad(z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_lr = 0.1\n",
    "d_lr = 0.5\n",
    "\n",
    "opt_flag = 1\n",
    "if opt_flag == 1\n",
    "    d_opt = Flux.setup(Adam(), d) \n",
    "elseif opt_flag == 2\n",
    "    d_opt = Flux.setup(Descent(d_lr), d) #lol is this right\n",
    "elseif opt_flag == 3\n",
    "    d_opt = Flux.setup(SGD(), d) #yeah idk anymore\n",
    "end\n",
    "\n",
    "gLoss_values = []\n",
    "dLoss_values = []\n",
    "g_epochs = 50\n",
    "d_epochs = 10\n",
    "epochs = 10\n",
    "\n",
    "for i in 1:epochs\n",
    "    t = time()\n",
    "    for j in 1:d_epochs\n",
    "        real = []\n",
    "        fake = []\n",
    "        for m in 1:batch_size\n",
    "            x = data_train[m] |> vec \n",
    "            push!(real, Int.(x))\n",
    "            z = pprior()\n",
    "            push!(fake, convertgd(z |> g |> measure))\n",
    "        end\n",
    "        data = collect(zip(real, fake)) \n",
    "        Flux.train!(dLoss, d, data, d_opt)\n",
    "        s = 0\n",
    "        for x in data\n",
    "            s += dLoss(d, x[1], x[2])        \n",
    "        end\n",
    "        push!(dLoss_values, s / batch_size)\n",
    "    end\n",
    "    for j in 1:g_epochs\n",
    "        s = 0\n",
    "        grads = []\n",
    "        for m in 1:batch_size\n",
    "            z = pprior()\n",
    "            push!(grads, finitediff_grad(z, ε))\n",
    "            s += gLoss(z)\n",
    "        end\n",
    "        dispatch!(-, g, mean(grads) * g_lr)\n",
    "        push!(gLoss_values, s / batch_size)\n",
    "    end\n",
    "    println(\"iteration $i done in: \", time() - t)\n",
    "end\n",
    "\n",
    "for j in 1:500\n",
    "    s = 0\n",
    "    grads = []\n",
    "    for m in 1:batch_size\n",
    "        z = pprior()\n",
    "        push!(grads, finitediff_grad(z, ε))\n",
    "        s += gLoss(z)\n",
    "    end\n",
    "    dispatch!(-, g, mean(grads) * g_lr)\n",
    "    #println(\"Generator loss for g_epoch $j:\", s / batch_size)\n",
    "    push!(gLoss_values, s / batch_size)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Generator loss\")\n",
    "\n",
    "# for l in gLoss_values\n",
    "#     println(l)\n",
    "# end\n",
    "\n",
    "x = 1:((g_epochs * epochs) + 500)\n",
    "plot(x, gLoss_values, title = \"Generator Loss\", linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Discriminator loss\")\n",
    "\n",
    "# for l in dLoss_values\n",
    "#     println(l)\n",
    "# end\n",
    "\n",
    "x = 1:(d_epochs * epochs)\n",
    "plot(x, dLoss_values, title = \"Discriminator Loss\", linewidth=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Discriminator\n",
    "\n",
    "d_fake = run_circuit(pprior())\n",
    "println(\"fake data: $d_fake\") #Should print a number close to 0\n",
    "\n",
    "index = rand(1:train)\n",
    "d_real = run_d(data_train[index])\n",
    "println(\"real data: $d_real\") #Should print a number close to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing the Generator\n",
    "\n",
    "nsamples = 16\n",
    "samples = []\n",
    "for i in 1:nsamples\n",
    "    reg = pprior()\n",
    "    s = reg |> g |> measure\n",
    "    push!(samples, convertgd(s))\n",
    "end\n",
    "plot([heatmap(rotl90(reshape(s, compress_size, compress_size)), color=:grays, colorbar = false, ticks = false) for s in samples]...) #Not sure if the reshaping is correct but whatever"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1",
   "language": "julia",
   "name": "julia-1.9"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
